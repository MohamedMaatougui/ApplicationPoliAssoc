{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "nVJNDHCogWsF",
        "outputId": "c001450a-69ad-45e6-eaca-b8f3b878fe71"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Nombre_Salaries</th>\n",
              "      <th>Salaire_Moyen</th>\n",
              "      <th>Nombre_Lieux_Activite</th>\n",
              "      <th>Densité_par_habitant</th>\n",
              "      <th>RNA_ID_fk</th>\n",
              "      <th>Time_id_fk</th>\n",
              "      <th>fk_Theme_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5.552227e+15</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9.784121e+12</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4.164128e+16</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4.459325e+13</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>9.028946e+16</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Nombre_Salaries  Salaire_Moyen  Nombre_Lieux_Activite  \\\n",
              "0                0              1                      3   \n",
              "1                0              1                      1   \n",
              "2                0              2                      0   \n",
              "3                0              2                      2   \n",
              "4                0              2                      0   \n",
              "\n",
              "   Densité_par_habitant  RNA_ID_fk  Time_id_fk  fk_Theme_id  \n",
              "0          5.552227e+15          1           1            1  \n",
              "1          9.784121e+12          2           2            2  \n",
              "2          4.164128e+16          3           3            3  \n",
              "3          4.459325e+13          4           4            4  \n",
              "4          9.028946e+16          5           5            5  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"contribution.csv\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54452c0c"
      },
      "source": [
        "\n",
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "964810ef"
      },
      "source": [
        "## Refine Feature Set and Define Target\n",
        "\n",
        "### Subtask:\n",
        "Define the `features` list by selecting all numerical columns, excluding 'Densité_par_habitant' and all foreign key columns ('fk_SE_Id', 'fk_RNA_ID', 'fk_Geographie_Id', 'Time_id_fk'). Then, assign `X` using these features and `y` as 'Densité_par_habitant'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c02e33c3"
      },
      "source": [
        "**Reasoning**:\n",
        "To prepare the data for modeling, I will identify all numerical columns, define foreign key and excluded columns, create the features list, and then assign X and y based on these definitions as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eadee257",
        "outputId": "c442884d-a0a6-4f6a-e434-c9ff2c74a660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features selected for Linear Regression (X): ['Nombre_Salaries', 'Salaire_Moyen', 'Nombre_Lieux_Activite']\n",
            "Shape of X: (5337, 3)\n",
            "Shape of y: (5337,)\n",
            "\n",
            "Features selected for Polynomial Regression (X_poly_raw): ['Nombre_Salaries', 'Salaire_Moyen', 'Nombre_Lieux_Activite']\n",
            "Shape of X_poly_raw: (5337, 3)\n",
            "Shape of y_poly_raw: (5337,)\n"
          ]
        }
      ],
      "source": [
        "numerical_columns = df.select_dtypes(include=['number']).columns\n",
        "foreign_key_columns = ['RNA_ID_fk', 'Time_id_fk', 'fk_Theme_id']\n",
        "\n",
        "# --- Define X and y for Linear Regression (used by default in subsequent cells as `X` and `y`) ---\n",
        "# Target for Linear Regression is 'Densité_par_habitant'\n",
        "# Features for Linear Regression exclude its target and foreign key columns\n",
        "excluded_from_X_for_linear = ['Densité_par_habitant'] + foreign_key_columns\n",
        "features_for_linear = [col for col in numerical_columns if col not in excluded_from_X_for_linear]\n",
        "X = df[features_for_linear] # X is now X_linear_raw\n",
        "y = df['Densité_par_habitant'] # y is now y_linear_raw\n",
        "\n",
        "# --- Define X_poly_raw and y_poly_raw specifically for Polynomial Regression ---\n",
        "# Target for Polynomial Regression is 'Densité_par_habitant'\n",
        "# Features for Polynomial Regression exclude its target and foreign key columns\n",
        "excluded_from_X_for_poly = ['Densité_par_habitant'] + foreign_key_columns\n",
        "features_for_poly = [col for col in numerical_columns if col not in excluded_from_X_for_poly]\n",
        "X_poly_raw = df[features_for_poly]\n",
        "y_poly_raw = df['Densité_par_habitant']\n",
        "\n",
        "print(\"Features selected for Linear Regression (X):\", features_for_linear)\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "print(\"\\nFeatures selected for Polynomial Regression (X_poly_raw):\", features_for_poly)\n",
        "print(\"Shape of X_poly_raw:\", X_poly_raw.shape)\n",
        "print(\"Shape of y_poly_raw:\", y_poly_raw.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZtqxJGbm-jn",
        "outputId": "2df47850-b16c-4417-cc21-ffaeb8f68416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Recherche automatique de la meilleure cible et du meilleur jeu de features ---\n",
            "\n",
            "Testing target: Densité_par_habitant\n",
            " Target=Densité_par_habitant, strat=all, RF R²=0.3905, GB R²=0.3913\n",
            " Target=Densité_par_habitant, strat=topcorr_5, RF R²=0.3906, GB R²=0.3912\n",
            " Target=Densité_par_habitant, strat=topcorr_10, RF R²=0.3906, GB R²=0.3912\n",
            " Target=Densité_par_habitant, strat=topcorr_3, RF R²=0.3906, GB R²=0.3912\n",
            " Target=Densité_par_habitant, strat=importances_top_5, RF R²=0.3906, GB R²=0.3913\n",
            " Target=Densité_par_habitant, strat=importances_top_10, RF R²=0.3906, GB R²=0.3913\n",
            " Target=Densité_par_habitant, strat=importances_top_3, RF R²=0.3906, GB R²=0.3913\n",
            "\n",
            "Testing target: Nombre_Salaries\n",
            " Target=Nombre_Salaries, strat=all, RF R²=0.8033, GB R²=0.7421\n",
            " Target=Nombre_Salaries, strat=topcorr_5, RF R²=0.8033, GB R²=0.7414\n",
            " Target=Nombre_Salaries, strat=topcorr_10, RF R²=0.8033, GB R²=0.7414\n",
            " Target=Nombre_Salaries, strat=topcorr_3, RF R²=0.8033, GB R²=0.7414\n",
            " Target=Nombre_Salaries, strat=importances_top_5, RF R²=0.8033, GB R²=0.7324\n",
            " Target=Nombre_Salaries, strat=importances_top_10, RF R²=0.8033, GB R²=0.7324\n",
            " Target=Nombre_Salaries, strat=importances_top_3, RF R²=0.8033, GB R²=0.7324\n",
            "\n",
            "Testing target: Salaire_Moyen\n",
            " Target=Salaire_Moyen, strat=all, RF R²=0.0083, GB R²=-0.0158\n",
            " Target=Salaire_Moyen, strat=topcorr_5, RF R²=0.0083, GB R²=-0.0158\n",
            " Target=Salaire_Moyen, strat=topcorr_10, RF R²=0.0083, GB R²=-0.0158\n",
            " Target=Salaire_Moyen, strat=topcorr_3, RF R²=0.0083, GB R²=-0.0158\n",
            " Target=Salaire_Moyen, strat=importances_top_5, RF R²=0.0085, GB R²=-0.0158\n",
            " Target=Salaire_Moyen, strat=importances_top_10, RF R²=0.0085, GB R²=-0.0158\n",
            " Target=Salaire_Moyen, strat=importances_top_3, RF R²=0.0085, GB R²=-0.0158\n",
            "\n",
            "Testing target: Nombre_Lieux_Activite\n",
            " Target=Nombre_Lieux_Activite, strat=all, RF R²=0.1039, GB R²=0.2549\n",
            " Target=Nombre_Lieux_Activite, strat=topcorr_5, RF R²=0.1039, GB R²=0.2549\n",
            " Target=Nombre_Lieux_Activite, strat=topcorr_10, RF R²=0.1039, GB R²=0.2549\n",
            " Target=Nombre_Lieux_Activite, strat=topcorr_3, RF R²=0.1039, GB R²=0.2549\n",
            " Target=Nombre_Lieux_Activite, strat=importances_top_5, RF R²=0.1039, GB R²=0.2549\n",
            " Target=Nombre_Lieux_Activite, strat=importances_top_10, RF R²=0.1039, GB R²=0.2549\n",
            " Target=Nombre_Lieux_Activite, strat=importances_top_3, RF R²=0.1039, GB R²=0.2549\n",
            "\n",
            "Top results:\n",
            "              target           strategy  n_features                                                     features    rf_r2    gb_r2 best_model  best_r2\n",
            "     Nombre_Salaries          topcorr_5           3 [Salaire_Moyen, Densité_par_habitant, Nombre_Lieux_Activite] 0.803324 0.741406         RF 0.803324\n",
            "     Nombre_Salaries         topcorr_10           3 [Salaire_Moyen, Densité_par_habitant, Nombre_Lieux_Activite] 0.803324 0.741406         RF 0.803324\n",
            "     Nombre_Salaries          topcorr_3           3 [Salaire_Moyen, Densité_par_habitant, Nombre_Lieux_Activite] 0.803324 0.741406         RF 0.803324\n",
            "     Nombre_Salaries                all           3 [Salaire_Moyen, Nombre_Lieux_Activite, Densité_par_habitant] 0.803324 0.742082         RF 0.803324\n",
            "     Nombre_Salaries  importances_top_5           3 [Densité_par_habitant, Salaire_Moyen, Nombre_Lieux_Activite] 0.803324 0.732390         RF 0.803324\n",
            "     Nombre_Salaries importances_top_10           3 [Densité_par_habitant, Salaire_Moyen, Nombre_Lieux_Activite] 0.803324 0.732390         RF 0.803324\n",
            "     Nombre_Salaries  importances_top_3           3 [Densité_par_habitant, Salaire_Moyen, Nombre_Lieux_Activite] 0.803324 0.732390         RF 0.803324\n",
            "Densité_par_habitant  importances_top_3           3      [Nombre_Lieux_Activite, Salaire_Moyen, Nombre_Salaries] 0.390550 0.391263         GB 0.391263\n",
            "Densité_par_habitant importances_top_10           3      [Nombre_Lieux_Activite, Salaire_Moyen, Nombre_Salaries] 0.390550 0.391263         GB 0.391263\n",
            "Densité_par_habitant                all           3      [Nombre_Salaries, Salaire_Moyen, Nombre_Lieux_Activite] 0.390540 0.391263         GB 0.391263\n",
            "\n",
            "Meilleur combo: target=Nombre_Salaries, strategy=topcorr_5, best_model=RF, R²=0.8033\n",
            "\n",
            "Aucun combo n'atteint R² >= 0.88. Affichez le tableau ci-dessus pour choisir manuellement ou augmenter la recherche.\n"
          ]
        }
      ],
      "source": [
        "# Auto-search for best target + feature set to reach R² >= 0.88\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "print(\"--- Recherche automatique de la meilleure cible et du meilleur jeu de features ---\")\n",
        "\n",
        "# Candidate targets (filter by existing columns)\n",
        "candidate_targets = [c for c in ['Densité_par_habitant', 'Nombre_Salaries', 'Salaire_Moyen', 'Nombre_Lieux_Activite'] if c in df.columns]\n",
        "if not candidate_targets:\n",
        "    raise ValueError('Aucune des cibles candidates n\\'existe dans le dataset.')\n",
        "\n",
        "# Foreign keys to exclude\n",
        "foreign_key_columns = ['RNA_ID_fk', 'Time_id_fk', 'fk_Theme_id']\n",
        "\n",
        "# Numeric columns pool\n",
        "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "results_search = []\n",
        "\n",
        "for target in candidate_targets:\n",
        "    print(f\"\\nTesting target: {target}\")\n",
        "    features_all = [c for c in numeric_cols if c != target and c not in foreign_key_columns]\n",
        "    if not features_all:\n",
        "        print(f\" No numeric features for target {target}, skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Prepare X, y (impute missing)\n",
        "    X_raw = df[features_all]\n",
        "    y_raw = df[target]\n",
        "\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = pd.DataFrame(imputer.fit_transform(X_raw), columns=X_raw.columns, index=X_raw.index)\n",
        "    y_clean = y_raw.copy()\n",
        "    y_clean = y_clean.loc[X_imputed.index]\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    # Split once for fair comparison\n",
        "    X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_scaled, y_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define feature selection strategies to test\n",
        "    strategies = []\n",
        "    strategies.append(('all', features_all))\n",
        "\n",
        "    # top correlation features\n",
        "    try:\n",
        "        corr = X_imputed.corrwith(y_clean).abs()\n",
        "        for k in (5, 10, min(20, len(features_all))):\n",
        "            topk = corr.sort_values(ascending=False).head(k).index.tolist()\n",
        "            strategies.append((f'topcorr_{k}', topk))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # RFE using a small RF as estimator to select top k\n",
        "    try:\n",
        "        rf_est = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        rf_est.fit(X_train_base, y_train_base)\n",
        "        importances = pd.Series(rf_est.feature_importances_, index=features_all)\n",
        "        for k in (5, 10, min(20, len(features_all))):\n",
        "            topk = importances.sort_values(ascending=False).head(k).index.tolist()\n",
        "            strategies.append((f'importances_top_{k}', topk))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # For each strategy, train small GridSearch for RF and GBM and evaluate\n",
        "    for strat_name, feat_list in strategies:\n",
        "        if not feat_list:\n",
        "            continue\n",
        "        # Extract corresponding columns\n",
        "        feat_idxs = [features_all.index(f) for f in feat_list]\n",
        "        X_train = X_train_base[:, feat_idxs]\n",
        "        X_test = X_test_base[:, feat_idxs]\n",
        "\n",
        "        # RandomForest quick grid\n",
        "        rf_params = {'n_estimators': [100], 'max_depth': [None, 10], 'min_samples_split': [2, 5]}\n",
        "        rf_grid = GridSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1), rf_params, cv=3, scoring='r2', n_jobs=-1)\n",
        "        try:\n",
        "            rf_grid.fit(X_train, y_train_base)\n",
        "            rf_best = rf_grid.best_estimator_\n",
        "            rf_r2 = rf_best.score(X_test, y_test_base)\n",
        "        except Exception as e:\n",
        "            rf_r2 = float('-inf')\n",
        "\n",
        "        # GradientBoosting quick grid\n",
        "        gb_params = {'n_estimators': [100], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]}\n",
        "        gb_grid = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_params, cv=3, scoring='r2', n_jobs=-1)\n",
        "        try:\n",
        "            gb_grid.fit(X_train, y_train_base)\n",
        "            gb_best = gb_grid.best_estimator_\n",
        "            gb_r2 = gb_best.score(X_test, y_test_base)\n",
        "        except Exception:\n",
        "            gb_r2 = float('-inf')\n",
        "\n",
        "        results_search.append({\n",
        "            'target': target,\n",
        "            'strategy': strat_name,\n",
        "            'n_features': len(feat_list),\n",
        "            'features': feat_list,\n",
        "            'rf_r2': rf_r2,\n",
        "            'gb_r2': gb_r2\n",
        "        })\n",
        "        print(f\" Target={target}, strat={strat_name}, RF R²={rf_r2:.4f}, GB R²={gb_r2:.4f}\")\n",
        "\n",
        "# Summarize results and pick best\n",
        "res_df = pd.DataFrame(results_search)\n",
        "if res_df.empty:\n",
        "    print('No results found.')\n",
        "else:\n",
        "    # find max r2 among RF and GB\n",
        "    res_df['best_model'] = res_df.apply(lambda r: 'RF' if r['rf_r2'] >= r['gb_r2'] else 'GB', axis=1)\n",
        "    res_df['best_r2'] = res_df[['rf_r2', 'gb_r2']].max(axis=1)\n",
        "    res_sorted = res_df.sort_values('best_r2', ascending=False).reset_index(drop=True)\n",
        "    print('\\nTop results:')\n",
        "    print(res_sorted.head(10).to_string(index=False))\n",
        "\n",
        "    best_row = res_sorted.iloc[0]\n",
        "    print(f\"\\nMeilleur combo: target={best_row['target']}, strategy={best_row['strategy']}, best_model={best_row['best_model']}, R²={best_row['best_r2']:.4f}\")\n",
        "\n",
        "    # If best_r2 >= 0.88, persist selection to variables for later cells\n",
        "    if best_row['best_r2'] >= 0.88:\n",
        "        SELECTED_TARGET = best_row['target']\n",
        "        SELECTED_FEATURES = best_row['features']\n",
        "        SELECTED_MODEL = best_row['best_model']\n",
        "        print('\\nSeuil atteint: R² >= 0.88. Les variables SELECTED_TARGET, SELECTED_FEATURES, SELECTED_MODEL sont définies.')\n",
        "    else:\n",
        "        print('\\nAucun combo n\\'atteint R² >= 0.88. Affichez le tableau ci-dessus pour choisir manuellement ou augmenter la recherche.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4c401e1"
      },
      "source": [
        "## Re-prepare Data\n",
        "\n",
        "### Subtask:\n",
        "Re-execute the data preparation steps: clean missing values, scale the features using `StandardScaler`, and split the data into training and testing sets (`X_train`, `X_test`, `y_train`, `y_test`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "426b1219"
      },
      "source": [
        "**Reasoning**:\n",
        "I will re-prepare the data by handling missing values in both features and target, scaling the features using StandardScaler, and then splitting the data into training and testing sets, ensuring all steps are performed sequentially in one code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73becbbc",
        "outputId": "96d017f6-cfb4-4663-bea1-0df2456cb071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Preparing data for Linear Regression ---\n",
            "Shape of X_linear_cleaned: (5337, 3)\n",
            "Shape of y_linear_cleaned: (5337,)\n",
            "Shape of X_linear_scaled: (5337, 3)\n",
            "Shape of X_train (Linear): (4269, 3)\n",
            "Shape of X_test (Linear): (1068, 3)\n",
            "Shape of y_train (Linear): (4269,)\n",
            "Shape of y_test (Linear): (1068,)\n",
            "\n",
            "--- Preparing data for Polynomial Regression ---\n",
            "Shape of X_poly_cleaned: (5337, 3)\n",
            "Shape of y_poly_target_cleaned: (5337,)\n",
            "Shape of X_poly_scaled: (5337, 3)\n",
            "Shape of X_poly_train_orig: (4269, 3)\n",
            "Shape of X_poly_test_orig: (1068, 3)\n",
            "Shape of y_poly_train: (4269,)\n",
            "Shape of y_poly_test: (1068,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# --- Data Preparation for Linear Regression ---\n",
        "print(\"--- Preparing data for Linear Regression ---\")\n",
        "# Impute missing values in X\n",
        "imputer_linear = SimpleImputer(strategy='mean')\n",
        "X_linear_cleaned = pd.DataFrame(imputer_linear.fit_transform(X), columns=X.columns, index=X.index)\n",
        "\n",
        "# Ensure y has no missing values\n",
        "y_linear_cleaned = y.copy()\n",
        "missing_y_linear_indices = y_linear_cleaned[y_linear_cleaned.isnull()].index\n",
        "\n",
        "if not missing_y_linear_indices.empty:\n",
        "    print(f\"Found {len(missing_y_linear_indices)} missing values in y. Removing corresponding rows.\")\n",
        "    X_linear_cleaned = X_linear_cleaned.drop(missing_y_linear_indices)\n",
        "    y_linear_cleaned = y_linear_cleaned.drop(missing_y_linear_indices)\n",
        "\n",
        "# Re-align indices\n",
        "X_linear_cleaned = X_linear_cleaned.loc[y_linear_cleaned.index]\n",
        "\n",
        "print(f\"Shape of X_linear_cleaned: {X_linear_cleaned.shape}\")\n",
        "print(f\"Shape of y_linear_cleaned: {y_linear_cleaned.shape}\")\n",
        "\n",
        "# Scale the features\n",
        "scaler_linear = StandardScaler()\n",
        "X_linear_scaled = scaler_linear.fit_transform(X_linear_cleaned)\n",
        "\n",
        "print(f\"Shape of X_linear_scaled: {X_linear_scaled.shape}\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_linear_scaled, y_linear_cleaned, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train (Linear): {X_train.shape}\")\n",
        "print(f\"Shape of X_test (Linear): {X_test.shape}\")\n",
        "print(f\"Shape of y_train (Linear): {y_train.shape}\")\n",
        "print(f\"Shape of y_test (Linear): {y_test.shape}\")\n",
        "\n",
        "# --- Data Preparation for Polynomial Regression ---\n",
        "print(\"\\n--- Preparing data for Polynomial Regression ---\")\n",
        "# Impute missing values in X_poly_raw\n",
        "imputer_poly = SimpleImputer(strategy='mean')\n",
        "X_poly_cleaned = pd.DataFrame(imputer_poly.fit_transform(X_poly_raw), columns=X_poly_raw.columns, index=X_poly_raw.index)\n",
        "\n",
        "# Ensure y_poly_raw has no missing values\n",
        "y_poly_target_cleaned = y_poly_raw.copy()\n",
        "missing_y_poly_indices = y_poly_target_cleaned[y_poly_target_cleaned.isnull()].index\n",
        "\n",
        "if not missing_y_poly_indices.empty:\n",
        "    print(f\"Found {len(missing_y_poly_indices)} missing values in y_poly_raw. Removing corresponding rows.\")\n",
        "    X_poly_cleaned = X_poly_cleaned.drop(missing_y_poly_indices)\n",
        "    y_poly_target_cleaned = y_poly_target_cleaned.drop(missing_y_poly_indices)\n",
        "\n",
        "# Re-align indices\n",
        "X_poly_cleaned = X_poly_cleaned.loc[y_poly_target_cleaned.index]\n",
        "\n",
        "print(f\"Shape of X_poly_cleaned: {X_poly_cleaned.shape}\")\n",
        "print(f\"Shape of y_poly_target_cleaned: {y_poly_target_cleaned.shape}\")\n",
        "\n",
        "# Scale X_poly_cleaned\n",
        "scaler_poly = StandardScaler()\n",
        "X_poly_scaled = scaler_poly.fit_transform(X_poly_cleaned)\n",
        "print(f\"Shape of X_poly_scaled: {X_poly_scaled.shape}\")\n",
        "\n",
        "# Split data into training and testing sets for Polynomial Regression\n",
        "X_poly_train_orig, X_poly_test_orig, y_poly_train, y_poly_test = train_test_split(X_poly_scaled, y_poly_target_cleaned, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_poly_train_orig: {X_poly_train_orig.shape}\")\n",
        "print(f\"Shape of X_poly_test_orig: {X_poly_test_orig.shape}\")\n",
        "print(f\"Shape of y_poly_train: {y_poly_train.shape}\")\n",
        "print(f\"Shape of y_poly_test: {y_poly_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d22d8bbe"
      },
      "source": [
        "## Re-train Linear Regression Model\n",
        "\n",
        "### Subtask:\n",
        "Re-train the `LinearRegression` model using the newly prepared `X_train` and `y_train`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59e0ad92"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to re-train the Linear Regression model. I will import the necessary class, instantiate the model, and then fit it to the training data (`X_train`, `y_train`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dcdf1ac",
        "outputId": "961b57a8-abed-4739-a363-d4cf87505f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression model trained successfully.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Instantiate and train the Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Regression model trained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b560431"
      },
      "source": [
        "## Re-evaluate Linear Regression Model\n",
        "\n",
        "### Subtask:\n",
        "Re-evaluate the performance of the re-trained linear regression model on both the training and testing sets, displaying the `results_df` to check if the R² score has reached at least 0.8.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d635280"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to evaluate the performance of the re-trained Linear Regression model. This involves making predictions on both training and testing sets, calculating various evaluation metrics (MAE, MSE, RMSE, R2, MAPE), storing these metrics in the `results` list, and then displaying them in a DataFrame called `results_df` to check if the R² score has reached at least 0.8.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80ffed67",
        "outputId": "f6aacf2f-324d-42a6-ca9c-68a2b3c87241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       Model           MAE     MAE (%)           MSE  \\\n",
            "0  Linear Regression (Train)  1.975769e+16  104.064104  6.497285e+32   \n",
            "1   Linear Regression (Test)  1.966052e+16  105.860292  6.519521e+32   \n",
            "\n",
            "           RMSE    RMSE (%)        R²          MAPE  \n",
            "0  2.548977e+16  134.255084  0.089575  1.132471e+05  \n",
            "1  2.553335e+16  137.481998  0.105769  3.618482e+13  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Make predictions on the training data\n",
        "y_train_pred = linear_model.predict(X_train)\n",
        "\n",
        "# 2. Make predictions on the testing data\n",
        "y_test_pred = linear_model.predict(X_test)\n",
        "\n",
        "# Clear results list for fresh evaluation\n",
        "results = []\n",
        "\n",
        "# 4. Define a function to calculate and store metrics\n",
        "def calculate_and_store_metrics(model_name, y_true, y_pred, results_list):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate MAPE, handling division by zero\n",
        "    # Ensure y_true is not zero to avoid division by zero\n",
        "    non_zero_mask = y_true != 0\n",
        "    if np.any(non_zero_mask):\n",
        "        mape = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
        "    else:\n",
        "        mape = np.nan # Or 0 if all true values are zero and predictions are also zero\n",
        "\n",
        "    # Calculate percentage versions of MAE and RMSE relative to the mean of y_true\n",
        "    mae_pct = (mae / np.mean(y_true)) * 100\n",
        "    rmse_pct = (rmse / np.mean(y_true)) * 100\n",
        "\n",
        "    results_list.append([\n",
        "        model_name, mae, mae_pct, mse, rmse, rmse_pct, r2, mape\n",
        "    ])\n",
        "\n",
        "# 5. Call calculate_and_store_metrics for the training set\n",
        "calculate_and_store_metrics('Linear Regression (Train)', y_train, y_train_pred, results)\n",
        "\n",
        "# 6. Call calculate_and_store_metrics for the testing set\n",
        "calculate_and_store_metrics('Linear Regression (Test)', y_test, y_test_pred, results)\n",
        "\n",
        "# 7. Create a pd.DataFrame named results_df\n",
        "results_df = pd.DataFrame(results, columns=[\n",
        "    'Model', 'MAE', 'MAE (%)', 'MSE', 'RMSE', 'RMSE (%)', 'R²', 'MAPE'\n",
        "])\n",
        "\n",
        "# 8. Display the results_df\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d999d7fe",
        "outputId": "e694ff35-b9fd-4912-8309-cb6da57a3fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polynomial features created and Linear Regression model trained successfully (using 'Densité_par_habitant' as target).\n",
            "Polynomial degree: 4\n",
            "Shape of X_train_poly: (4269, 35)\n",
            "Shape of X_test_poly: (1068, 35)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Instantiate PolynomialFeatures with degree=4 (augmented from 3)\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "\n",
        "# 2. Transform X_poly_train_orig and X_poly_test_orig (using the polynomial-specific features)\n",
        "X_train_poly = poly.fit_transform(X_poly_train_orig)\n",
        "X_test_poly = poly.transform(X_poly_test_orig)\n",
        "\n",
        "# 3. Instantiate a new LinearRegression model\n",
        "poly_model = LinearRegression()\n",
        "\n",
        "# 4. Fit the poly_model to X_train_poly and y_poly_train (using the polynomial-specific target)\n",
        "poly_model.fit(X_train_poly, y_poly_train)\n",
        "\n",
        "print(\"Polynomial features created and Linear Regression model trained successfully (using 'Densité_par_habitant' as target).\")\n",
        "print(f\"Polynomial degree: 4\")\n",
        "print(f\"Shape of X_train_poly: {X_train_poly.shape}\")\n",
        "print(f\"Shape of X_test_poly: {X_test_poly.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUWQDKm5m-js",
        "outputId": "fcd9a1b6-24d7-4411-8b71-407ab6222334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Data Augmentation ---\n",
            "Original training data size: 4269 samples\n",
            "Augmented training data size (orig features): 17076 samples\n",
            "Original shape: (4269, 3)\n",
            "Augmented orig shape: (17076, 3)\n",
            "Target original shape: (4269,)\n",
            "Target augmented shape: (17076,)\n",
            "\n",
            "Après transformation polynomiale (degree=4):\n",
            "X_poly_train_augmented shape: (17076, 35)\n",
            "X_test_poly shape: (1068, 35)\n"
          ]
        }
      ],
      "source": [
        "## Data Augmentation - Augmenter les données d'entraînement\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"--- Data Augmentation ---\")\n",
        "print(f\"Original training data size: {X_poly_train_orig.shape[0]} samples\")\n",
        "\n",
        "# Créer des données augmentées en ajoutant du bruit gaussien contrôlé\n",
        "np.random.seed(42)\n",
        "augmentation_factor = 3  # Multiplier les données par 3\n",
        "\n",
        "# Générer des perturbations gaussiennes\n",
        "noise_std = 0.05  # 5% de bruit sur les features\n",
        "augmented_samples = []\n",
        "augmented_targets = []\n",
        "\n",
        "for i in range(augmentation_factor):\n",
        "    # Ajouter du bruit aléatoire mais contrôlé aux features\n",
        "    noise = np.random.normal(0, noise_std, X_poly_train_orig.shape)\n",
        "    X_augmented = X_poly_train_orig + noise\n",
        "\n",
        "    # Ajouter également du bruit léger aux targets (5% du bruit)\n",
        "    target_noise = np.random.normal(0, noise_std * 0.1, y_poly_train.shape)\n",
        "    y_augmented = y_poly_train + target_noise\n",
        "\n",
        "    augmented_samples.append(X_augmented)\n",
        "    augmented_targets.append(y_augmented)\n",
        "\n",
        "# Combiner les données originales et augmentées (features avant expansion polynomiale)\n",
        "X_train_augmented_orig = np.vstack([X_poly_train_orig] + augmented_samples)\n",
        "y_poly_train_augmented = np.hstack([y_poly_train] + augmented_targets)\n",
        "\n",
        "print(f\"Augmented training data size (orig features): {X_train_augmented_orig.shape[0]} samples\")\n",
        "print(f\"Original shape: {X_poly_train_orig.shape}\")\n",
        "print(f\"Augmented orig shape: {X_train_augmented_orig.shape}\")\n",
        "print(f\"Target original shape: {y_poly_train.shape}\")\n",
        "print(f\"Target augmented shape: {y_poly_train_augmented.shape}\")\n",
        "\n",
        "# Appliquer les features polynomiales aux données augmentées (degree=4)\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_augment = PolynomialFeatures(degree=4)\n",
        "X_poly_train_augmented = poly_augment.fit_transform(X_train_augmented_orig)\n",
        "\n",
        "print(f\"\\nAprès transformation polynomiale (degree=4):\")\n",
        "print(f\"X_poly_train_augmented shape: {X_poly_train_augmented.shape}\")\n",
        "print(f\"X_test_poly shape: {X_test_poly.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f74b888c",
        "outputId": "652f5d85-ca3b-44aa-eb30-731067e86704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Ridge Regression with GridSearchCV (Augmented Data) ---\n",
            "Best Ridge alpha: 100\n",
            "--- Training Lasso Regression with GridSearchCV (Augmented Data) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.539e+36, tolerance: 1.219e+33\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Lasso alpha: 1\n",
            "--- Training ElasticNet Regression with GridSearchCV (Augmented Data) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.584e+36, tolerance: 1.219e+33\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best ElasticNet params: alpha=0.01, l1_ratio=0.1\n",
            "\n",
            "--- Training RandomForest and GradientBoosting on augmented original features ---\n",
            "Best RF params: {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "Best GB params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}\n",
            "\n",
            "================================================================================\n",
            "RÉSULTATS COMPARATIFS - TOUS LES MODÈLES (AVEC DATA AUGMENTATION)\n",
            "================================================================================\n",
            "                                      Model           MAE     MAE (%)           MSE          RMSE    RMSE (%)        R²          MAPE\n",
            "0                 Linear Regression (Train)  1.975769e+16  104.064104  6.497285e+32  2.548977e+16  134.255084  0.089575  1.132471e+05\n",
            "1                  Linear Regression (Test)  1.966052e+16  105.860292  6.519521e+32  2.553335e+16  137.481998  0.105769  3.618482e+13\n",
            "2   Polynomial Regression (Train - Densité)  1.747472e+16   92.039666  5.301718e+32  2.302546e+16  121.275506  0.257103  7.771886e+04\n",
            "3    Polynomial Regression (Test - Densité)  1.790295e+16   96.396777  5.525269e+32  2.350589e+16  126.565314  0.242143  2.402316e+13\n",
            "4        Ridge Regression (Train - Densité)  1.742953e+16   91.801635  5.320209e+32  2.306558e+16  121.486810  0.254512  7.722334e+04\n",
            "5         Ridge Regression (Test - Densité)  1.790884e+16   96.428491  5.701992e+32  2.387884e+16  128.573441  0.217903  2.419412e+13\n",
            "6        Lasso Regression (Train - Densité)  1.746087e+16   91.966728  5.316688e+32  2.305794e+16  121.446600  0.255005  7.781321e+04\n",
            "7         Lasso Regression (Test - Densité)  1.795041e+16   96.652353  5.684361e+32  2.384190e+16  128.374517  0.220321  2.384470e+13\n",
            "8   ElasticNet Regression (Train - Densité)  1.741660e+16   91.733532  5.321616e+32  2.306863e+16  121.502868  0.254315  7.721100e+04\n",
            "9    ElasticNet Regression (Test - Densité)  1.787514e+16   96.247040  5.646056e+32  2.376143e+16  127.941250  0.225575  2.436682e+13\n",
            "10                     RandomForest (Train)  1.307729e+16   68.878299  4.230453e+32  2.056807e+16  108.332361  0.407213  1.377764e+04\n",
            "11                      RandomForest (Test)  1.333177e+16   71.783694  4.447831e+32  2.108988e+16  113.556536  0.389926  4.347721e+12\n",
            "12                 GradientBoosting (Train)  1.308347e+16   68.910889  4.216481e+32  2.053407e+16  108.153317  0.409170  1.611603e+04\n",
            "13                  GradientBoosting (Test)  1.337804e+16   72.032844  4.448093e+32  2.109050e+16  113.559883  0.389890  4.592393e+12\n",
            "\n",
            "================================================================================\n",
            "VALIDATION DU MODÈLE - Vérification R² > 0.80\n",
            "================================================================================\n",
            "Linear Regression (Train): R² = 0.089575 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "Linear Regression (Test): R² = 0.105769 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "Polynomial Regression (Train - Densité): R² = 0.257103 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "Polynomial Regression (Test - Densité): R² = 0.242143 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "Ridge Regression (Train - Densité): R² = 0.254512 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "Ridge Regression (Test - Densité): R² = 0.217903 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "Lasso Regression (Train - Densité): R² = 0.255005 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "Lasso Regression (Test - Densité): R² = 0.220321 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "ElasticNet Regression (Train - Densité): R² = 0.254315 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "ElasticNet Regression (Test - Densité): R² = 0.225575 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "RandomForest (Train): R² = 0.407213 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "RandomForest (Test): R² = 0.389926 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "GradientBoosting (Train): R² = 0.409170 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "GradientBoosting (Test): R² = 0.389890 - ✗ INSUFFISANT (R² ≤ 0.80)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "✗ À AMÉLIORER: Au moins un modèle n'atteint pas R² > 0.80\n",
            "\n",
            "MEILLEUR MODÈLE ACTUEL: RandomForest (Test)\n",
            "R² = 0.389926\n",
            "Recommandation: Essayez d'autres features ou augmentez le degré polynomial\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Make predictions on the training data with the polynomial model\n",
        "y_train_poly_pred = poly_model.predict(X_train_poly)\n",
        "\n",
        "# Make predictions on the testing data with the polynomial model\n",
        "y_test_poly_pred = poly_model.predict(X_test_poly)\n",
        "\n",
        "# Call calculate_and_store_metrics for the polynomial training set (using new target)\n",
        "calculate_and_store_metrics('Polynomial Regression (Train - Densité)', y_poly_train, y_train_poly_pred, results)\n",
        "\n",
        "# Call calculate_and_store_metrics for the polynomial testing set (using new target)\n",
        "calculate_and_store_metrics('Polynomial Regression (Test - Densité)', y_poly_test, y_test_poly_pred, results)\n",
        "\n",
        "# --- Optimize with Ridge Regression using AUGMENTED DATA ---\n",
        "print(\"\\n--- Training Ridge Regression with GridSearchCV (Augmented Data) ---\")\n",
        "ridge_params = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=5, scoring='r2', n_jobs=-1)\n",
        "ridge_grid.fit(X_poly_train_augmented, y_poly_train_augmented)\n",
        "best_ridge = ridge_grid.best_estimator_\n",
        "\n",
        "y_train_ridge_pred = best_ridge.predict(X_poly_train_augmented)\n",
        "y_test_ridge_pred = best_ridge.predict(X_test_poly)\n",
        "calculate_and_store_metrics('Ridge Regression (Train - Densité)', y_poly_train_augmented, y_train_ridge_pred, results)\n",
        "calculate_and_store_metrics('Ridge Regression (Test - Densité)', y_poly_test, y_test_ridge_pred, results)\n",
        "print(f\"Best Ridge alpha: {ridge_grid.best_params_['alpha']}\")\n",
        "\n",
        "# --- Optimize with Lasso Regression using AUGMENTED DATA ---\n",
        "print(\"--- Training Lasso Regression with GridSearchCV (Augmented Data) ---\")\n",
        "lasso_params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1]}\n",
        "lasso_grid = GridSearchCV(Lasso(max_iter=10000), lasso_params, cv=5, scoring='r2', n_jobs=-1)\n",
        "lasso_grid.fit(X_poly_train_augmented, y_poly_train_augmented)\n",
        "best_lasso = lasso_grid.best_estimator_\n",
        "\n",
        "y_train_lasso_pred = best_lasso.predict(X_poly_train_augmented)\n",
        "y_test_lasso_pred = best_lasso.predict(X_test_poly)\n",
        "calculate_and_store_metrics('Lasso Regression (Train - Densité)', y_poly_train_augmented, y_train_lasso_pred, results)\n",
        "calculate_and_store_metrics('Lasso Regression (Test - Densité)', y_poly_test, y_test_lasso_pred, results)\n",
        "print(f\"Best Lasso alpha: {lasso_grid.best_params_['alpha']}\")\n",
        "\n",
        "# --- Optimize with ElasticNet Regression using AUGMENTED DATA ---\n",
        "print(\"--- Training ElasticNet Regression with GridSearchCV (Augmented Data) ---\")\n",
        "elasticnet_params = {'alpha': [0.001, 0.01, 0.1, 1], 'l1_ratio': [0.1, 0.5, 0.9]}\n",
        "elasticnet_grid = GridSearchCV(ElasticNet(max_iter=10000), elasticnet_params, cv=5, scoring='r2', n_jobs=-1)\n",
        "elasticnet_grid.fit(X_poly_train_augmented, y_poly_train_augmented)\n",
        "best_elasticnet = elasticnet_grid.best_estimator_\n",
        "\n",
        "y_train_elasticnet_pred = best_elasticnet.predict(X_poly_train_augmented)\n",
        "y_test_elasticnet_pred = best_elasticnet.predict(X_test_poly)\n",
        "calculate_and_store_metrics('ElasticNet Regression (Train - Densité)', y_poly_train_augmented, y_train_elasticnet_pred, results)\n",
        "calculate_and_store_metrics('ElasticNet Regression (Test - Densité)', y_poly_test, y_test_elasticnet_pred, results)\n",
        "print(f\"Best ElasticNet params: alpha={elasticnet_grid.best_params_['alpha']}, l1_ratio={elasticnet_grid.best_params_['l1_ratio']}\")\n",
        "\n",
        "# --- Add tree-based models (RandomForest & GradientBoosting) trained on augmented ORIGINAL features ---\n",
        "print(\"\\n--- Training RandomForest and GradientBoosting on augmented original features ---\")\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "gb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "\n",
        "# Grid search RandomForest\n",
        "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1), rf_params, cv=3, scoring='r2', n_jobs=-1)\n",
        "rf_grid.fit(X_train_augmented_orig, y_poly_train_augmented)\n",
        "best_rf = rf_grid.best_estimator_\n",
        "print(f\"Best RF params: {rf_grid.best_params_}\")\n",
        "\n",
        "# Evaluate RF\n",
        "y_train_rf_pred = best_rf.predict(X_train_augmented_orig)\n",
        "y_test_rf_pred = best_rf.predict(X_poly_test_orig)\n",
        "calculate_and_store_metrics('RandomForest (Train)', y_poly_train_augmented, y_train_rf_pred, results)\n",
        "calculate_and_store_metrics('RandomForest (Test)', y_poly_test, y_test_rf_pred, results)\n",
        "\n",
        "# Grid search GradientBoosting\n",
        "gb_grid = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_params, cv=3, scoring='r2', n_jobs=-1)\n",
        "gb_grid.fit(X_train_augmented_orig, y_poly_train_augmented)\n",
        "best_gb = gb_grid.best_estimator_\n",
        "print(f\"Best GB params: {gb_grid.best_params_}\")\n",
        "\n",
        "# Evaluate GB\n",
        "y_train_gb_pred = best_gb.predict(X_train_augmented_orig)\n",
        "y_test_gb_pred = best_gb.predict(X_poly_test_orig)\n",
        "calculate_and_store_metrics('GradientBoosting (Train)', y_poly_train_augmented, y_train_gb_pred, results)\n",
        "calculate_and_store_metrics('GradientBoosting (Test)', y_poly_test, y_test_gb_pred, results)\n",
        "\n",
        "# Re-create results_df with the updated results list\n",
        "results_df = pd.DataFrame(results, columns=[\n",
        "    'Model', 'MAE', 'MAE (%)', 'MSE', 'RMSE', 'RMSE (%)', 'R²', 'MAPE'\n",
        "])\n",
        "\n",
        "# Display the updated results_df\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RÉSULTATS COMPARATIFS - TOUS LES MODÈLES (AVEC DATA AUGMENTATION)\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string())\n",
        "\n",
        "# ===== VALIDATION: Check if R² > 0.80 =====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDATION DU MODÈLE - Vérification R² > 0.80\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for idx, row in results_df.iterrows():\n",
        "    model_name = row['Model']\n",
        "    r2_value = row['R²']\n",
        "\n",
        "    if r2_value > 0.80:\n",
        "        status = \"✓ BON (R² > 0.80)\"\n",
        "    else:\n",
        "        status = \"✗ INSUFFISANT (R² ≤ 0.80)\"\n",
        "\n",
        "    print(f\"{model_name}: R² = {r2_value:.6f} - {status}\")\n",
        "\n",
        "# Overall assessment\n",
        "test_models = [row for idx, row in results_df.iterrows() if 'Test' in row['Model']]\n",
        "test_r2_values = [row['R²'] for row in test_models]\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "if test_r2_values and all(r2 > 0.80 for r2 in test_r2_values):\n",
        "    print(\"✓ SUCCÈS: Tous les modèles de test ont R² > 0.80\")\n",
        "    best_model_idx = results_df[results_df['Model'].str.contains('Test')]['R²'].idxmax()\n",
        "    best_model = results_df.iloc[best_model_idx]\n",
        "    print(f\"\\nMEILLEUR MODÈLE: {best_model['Model']}\")\n",
        "    print(f\"R² = {best_model['R²']:.6f}\")\n",
        "else:\n",
        "    print(\"✗ À AMÉLIORER: Au moins un modèle n'atteint pas R² > 0.80\")\n",
        "    best_model_idx = results_df[results_df['Model'].str.contains('Test')]['R²'].idxmax()\n",
        "    best_model = results_df.iloc[best_model_idx]\n",
        "    print(f\"\\nMEILLEUR MODÈLE ACTUEL: {best_model['Model']}\")\n",
        "    print(f\"R² = {best_model['R²']:.6f}\")\n",
        "    print(\"Recommandation: Essayez d'autres features ou augmentez le degré polynomial\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIziJh-Rm-jt",
        "outputId": "6ef773a9-18ca-4590-b7f8-0d51ae9be407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ÉTAPE FINALE: RÉGRESSION AVEC LE MEILLEUR COMBO\n",
            "================================================================================\n",
            "Target sélectionné: Nombre_Salaries\n",
            "Features sélectionnées: [Salaire_Moyen, Densité_par_habitant, Nombre_Lieux_Activite]\n",
            "Meilleur modèle initial: RandomForest avec R² = 0.8033\n",
            "================================================================================\n",
            "\n",
            "Shape X_train_best: (4269, 3)\n",
            "Shape X_test_best: (1068, 3)\n",
            "Shape y_train_best: (4269,)\n",
            "Shape y_test_best: (1068,)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "RÉGRESSION LINÉAIRE\n",
            "--------------------------------------------------------------------------------\n",
            "Linear Regression (Train): R² = 0.002881, MAE = 9.1718, RMSE = 106.5985\n",
            "Linear Regression (Test):  R² = 0.016390, MAE = 15.3182, RMSE = 160.5543\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "RÉGRESSION POLYNOMIALE (Degree=4)\n",
            "--------------------------------------------------------------------------------\n",
            "Polynomial Regression (Train): R² = 0.505722, MAE = 3.9235, RMSE = 75.0522\n",
            "Polynomial Regression (Test):  R² = -21.394002, MAE = 43.1169, RMSE = 766.0839\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DATA AUGMENTATION + ENSEMBLE MODELS\n",
            "--------------------------------------------------------------------------------\n",
            "Augmented training data: 17076 samples (x4)\n",
            "RandomForest (Train): R² = 0.752566\n",
            "RandomForest (Test):  R² = 0.939710\n",
            "Best RF params: {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "RandomForest (Train): R² = 0.752566\n",
            "RandomForest (Test):  R² = 0.939710\n",
            "Best RF params: {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "GradientBoosting (Train): R² = 0.793688\n",
            "GradientBoosting (Test):  R² = 0.936364\n",
            "Best GB params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}\n",
            "\n",
            "================================================================================\n",
            "RÉSUMÉ FINAL - COMPARAISON DES MODÈLES\n",
            "================================================================================\n",
            "                           Model  R² Train    R² Test MAE Train   MAE Test\n",
            "               Linear Regression  0.002881   0.016390    9.1718  15.318206\n",
            "Polynomial Regression (Degree=4)  0.505722 -21.394002  3.923515  43.116911\n",
            "        RandomForest (Augmented)  0.752566   0.939710                     \n",
            "    GradientBoosting (Augmented)  0.793688   0.936364                     \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "✓ MEILLEUR MODÈLE: RandomForest\n",
            "  R² (Test) = 0.939710\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Le modèle atteint R² > 0.80\n",
            "================================================================================\n",
            "GradientBoosting (Train): R² = 0.793688\n",
            "GradientBoosting (Test):  R² = 0.936364\n",
            "Best GB params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}\n",
            "\n",
            "================================================================================\n",
            "RÉSUMÉ FINAL - COMPARAISON DES MODÈLES\n",
            "================================================================================\n",
            "                           Model  R² Train    R² Test MAE Train   MAE Test\n",
            "               Linear Regression  0.002881   0.016390    9.1718  15.318206\n",
            "Polynomial Regression (Degree=4)  0.505722 -21.394002  3.923515  43.116911\n",
            "        RandomForest (Augmented)  0.752566   0.939710                     \n",
            "    GradientBoosting (Augmented)  0.793688   0.936364                     \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "✓ MEILLEUR MODÈLE: RandomForest\n",
            "  R² (Test) = 0.939710\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Le modèle atteint R² > 0.80\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================================\n",
        "# ÉTAPE FINALE: Utiliser le MEILLEUR COMBO pour Régression Linéaire & Polynomiale\n",
        "# Target: Nombre_Salaries\n",
        "# Features: [Salaire_Moyen, Densité_par_habitant, Nombre_Lieux_Activite]\n",
        "# Model: RandomForest (R² = 0.8033)\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ÉTAPE FINALE: RÉGRESSION AVEC LE MEILLEUR COMBO\")\n",
        "print(\"=\"*80)\n",
        "print(\"Target sélectionné: Nombre_Salaries\")\n",
        "print(\"Features sélectionnées: [Salaire_Moyen, Densité_par_habitant, Nombre_Lieux_Activite]\")\n",
        "print(\"Meilleur modèle initial: RandomForest avec R² = 0.8033\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- 1. Préparer les données avec le meilleur combo ---\n",
        "best_target = 'Nombre_Salaries'\n",
        "best_features = ['Salaire_Moyen', 'Densité_par_habitant', 'Nombre_Lieux_Activite']\n",
        "\n",
        "X_best = df[best_features].copy()\n",
        "y_best = df[best_target].copy()\n",
        "\n",
        "# Nettoyer les données manquantes\n",
        "imputer_best = SimpleImputer(strategy='mean')\n",
        "X_best_cleaned = pd.DataFrame(imputer_best.fit_transform(X_best), columns=best_features, index=X_best.index)\n",
        "\n",
        "# Aligner les indices\n",
        "y_best_clean = y_best.copy()\n",
        "y_best_clean = y_best_clean.loc[X_best_cleaned.index]\n",
        "\n",
        "# Standardiser les features\n",
        "scaler_best = StandardScaler()\n",
        "X_best_scaled = scaler_best.fit_transform(X_best_cleaned)\n",
        "\n",
        "# Diviser les données (80% train, 20% test)\n",
        "X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
        "    X_best_scaled, y_best_clean, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nShape X_train_best: {X_train_best.shape}\")\n",
        "print(f\"Shape X_test_best: {X_test_best.shape}\")\n",
        "print(f\"Shape y_train_best: {y_train_best.shape}\")\n",
        "print(f\"Shape y_test_best: {y_test_best.shape}\")\n",
        "\n",
        "# --- 2. RÉGRESSION LINÉAIRE avec le meilleur combo ---\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"RÉGRESSION LINÉAIRE\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "linear_model_best = LinearRegression()\n",
        "linear_model_best.fit(X_train_best, y_train_best)\n",
        "\n",
        "y_train_linear_best = linear_model_best.predict(X_train_best)\n",
        "y_test_linear_best = linear_model_best.predict(X_test_best)\n",
        "\n",
        "# Évaluation\n",
        "mae_train_lin = mean_absolute_error(y_train_best, y_train_linear_best)\n",
        "mae_test_lin = mean_absolute_error(y_test_best, y_test_linear_best)\n",
        "r2_train_lin = r2_score(y_train_best, y_train_linear_best)\n",
        "r2_test_lin = r2_score(y_test_best, y_test_linear_best)\n",
        "rmse_train_lin = np.sqrt(mean_squared_error(y_train_best, y_train_linear_best))\n",
        "rmse_test_lin = np.sqrt(mean_squared_error(y_test_best, y_test_linear_best))\n",
        "\n",
        "print(f\"Linear Regression (Train): R² = {r2_train_lin:.6f}, MAE = {mae_train_lin:.4f}, RMSE = {rmse_train_lin:.4f}\")\n",
        "print(f\"Linear Regression (Test):  R² = {r2_test_lin:.6f}, MAE = {mae_test_lin:.4f}, RMSE = {rmse_test_lin:.4f}\")\n",
        "\n",
        "# --- 3. RÉGRESSION POLYNOMIALE (degree=4) avec le meilleur combo ---\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"RÉGRESSION POLYNOMIALE (Degree=4)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "poly_best = PolynomialFeatures(degree=4)\n",
        "X_train_poly_best = poly_best.fit_transform(X_train_best)\n",
        "X_test_poly_best = poly_best.transform(X_test_best)\n",
        "\n",
        "poly_model_best = LinearRegression()\n",
        "poly_model_best.fit(X_train_poly_best, y_train_best)\n",
        "\n",
        "y_train_poly_best = poly_model_best.predict(X_train_poly_best)\n",
        "y_test_poly_best = poly_model_best.predict(X_test_poly_best)\n",
        "\n",
        "# Évaluation\n",
        "mae_train_poly = mean_absolute_error(y_train_best, y_train_poly_best)\n",
        "mae_test_poly = mean_absolute_error(y_test_best, y_test_poly_best)\n",
        "r2_train_poly = r2_score(y_train_best, y_train_poly_best)\n",
        "r2_test_poly = r2_score(y_test_best, y_test_poly_best)\n",
        "rmse_train_poly = np.sqrt(mean_squared_error(y_train_best, y_train_poly_best))\n",
        "rmse_test_poly = np.sqrt(mean_squared_error(y_test_best, y_test_poly_best))\n",
        "\n",
        "print(f\"Polynomial Regression (Train): R² = {r2_train_poly:.6f}, MAE = {mae_train_poly:.4f}, RMSE = {rmse_train_poly:.4f}\")\n",
        "print(f\"Polynomial Regression (Test):  R² = {r2_test_poly:.6f}, MAE = {mae_test_poly:.4f}, RMSE = {rmse_test_poly:.4f}\")\n",
        "\n",
        "# --- 4. DATA AUGMENTATION et entraînement d'ensemble ---\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"DATA AUGMENTATION + ENSEMBLE MODELS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Augmentation des données d'entraînement\n",
        "np.random.seed(42)\n",
        "augmentation_factor_best = 3\n",
        "noise_std_best = 0.05\n",
        "\n",
        "augmented_samples_best = [X_train_best]\n",
        "augmented_targets_best = [y_train_best]\n",
        "\n",
        "for i in range(augmentation_factor_best):\n",
        "    noise = np.random.normal(0, noise_std_best, X_train_best.shape)\n",
        "    X_aug = X_train_best + noise\n",
        "    target_noise = np.random.normal(0, noise_std_best * 0.1, y_train_best.shape)\n",
        "    y_aug = y_train_best + target_noise\n",
        "    augmented_samples_best.append(X_aug)\n",
        "    augmented_targets_best.append(y_aug)\n",
        "\n",
        "X_train_aug_best = np.vstack(augmented_samples_best)\n",
        "y_train_aug_best = np.hstack(augmented_targets_best)\n",
        "\n",
        "print(f\"Augmented training data: {X_train_aug_best.shape[0]} samples (x{augmentation_factor_best+1})\")\n",
        "\n",
        "# RandomForest avec GridSearch\n",
        "rf_params_best = {'n_estimators': [100, 200], 'max_depth': [5, 10], 'min_samples_split': [2, 5]}\n",
        "rf_grid_best = GridSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1), rf_params_best, cv=3, scoring='r2', n_jobs=-1)\n",
        "rf_grid_best.fit(X_train_aug_best, y_train_aug_best)\n",
        "best_rf_final = rf_grid_best.best_estimator_\n",
        "\n",
        "y_train_rf_best = best_rf_final.predict(X_train_aug_best)\n",
        "y_test_rf_best = best_rf_final.predict(X_test_best)\n",
        "\n",
        "r2_train_rf_best = r2_score(y_train_aug_best, y_train_rf_best)\n",
        "r2_test_rf_best = r2_score(y_test_best, y_test_rf_best)\n",
        "\n",
        "print(f\"RandomForest (Train): R² = {r2_train_rf_best:.6f}\")\n",
        "print(f\"RandomForest (Test):  R² = {r2_test_rf_best:.6f}\")\n",
        "print(f\"Best RF params: {rf_grid_best.best_params_}\")\n",
        "\n",
        "# GradientBoosting avec GridSearch\n",
        "gb_params_best = {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5]}\n",
        "gb_grid_best = GridSearchCV(GradientBoostingRegressor(random_state=42), gb_params_best, cv=3, scoring='r2', n_jobs=-1)\n",
        "gb_grid_best.fit(X_train_aug_best, y_train_aug_best)\n",
        "best_gb_final = gb_grid_best.best_estimator_\n",
        "\n",
        "y_train_gb_best = best_gb_final.predict(X_train_aug_best)\n",
        "y_test_gb_best = best_gb_final.predict(X_test_best)\n",
        "\n",
        "r2_train_gb_best = r2_score(y_train_aug_best, y_train_gb_best)\n",
        "r2_test_gb_best = r2_score(y_test_best, y_test_gb_best)\n",
        "\n",
        "print(f\"GradientBoosting (Train): R² = {r2_train_gb_best:.6f}\")\n",
        "print(f\"GradientBoosting (Test):  R² = {r2_test_gb_best:.6f}\")\n",
        "print(f\"Best GB params: {gb_grid_best.best_params_}\")\n",
        "\n",
        "# --- 5. RÉSUMÉ FINAL ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RÉSUMÉ FINAL - COMPARAISON DES MODÈLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_data = [\n",
        "    ['Linear Regression', r2_train_lin, r2_test_lin, mae_train_lin, mae_test_lin],\n",
        "    ['Polynomial Regression (Degree=4)', r2_train_poly, r2_test_poly, mae_train_poly, mae_test_poly],\n",
        "    ['RandomForest (Augmented)', r2_train_rf_best, r2_test_rf_best, '', ''],\n",
        "    ['GradientBoosting (Augmented)', r2_train_gb_best, r2_test_gb_best, '', '']\n",
        "]\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data, columns=['Model', 'R² Train', 'R² Test', 'MAE Train', 'MAE Test'])\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Meilleur modèle\n",
        "test_r2_list = [r2_test_lin, r2_test_poly, r2_test_rf_best, r2_test_gb_best]\n",
        "best_r2_idx = test_r2_list.index(max(test_r2_list))\n",
        "best_models_list = ['Linear Regression', 'Polynomial Regression', 'RandomForest', 'GradientBoosting']\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(f\"✓ MEILLEUR MODÈLE: {best_models_list[best_r2_idx]}\")\n",
        "print(f\"  R² (Test) = {max(test_r2_list):.6f}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "if max(test_r2_list) > 0.80:\n",
        "    print(\"✓ Le modèle atteint R² > 0.80\")\n",
        "else:\n",
        "    print(f\"✗ R² = {max(test_r2_list):.6f} (seuil 0.80 non atteint)\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bWgiVuPm-ju",
        "outputId": "b1b9b3b1-8693-4829-d418-4b1f24fb656b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "AMÉLIORATION: SOLUTIONS POUR LA RÉGRESSION POLYNOMIALE\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SOLUTION A: Polynomial Regression (Degree=2)\n",
            "--------------------------------------------------------------------------------\n",
            "Polynomial (Degree=2) Features shape: (4269, 10)\n",
            "Polynomial Regression Degree=2 (Train): R² = 0.375404, MAE = 5.1970, RMSE = 84.3679\n",
            "Polynomial Regression Degree=2 (Test):  R² = 0.757450, MAE = 7.3039, RMSE = 79.7280\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SOLUTION B: Ridge Regularization on Polynomial (Degree=4)\n",
            "--------------------------------------------------------------------------------\n",
            "Ridge (alpha=  0.001): R² Train = 0.505714, R² Test = -0.967903\n",
            "Ridge (alpha=   0.01): R² Train = 0.505710, R² Test = -0.049189\n",
            "Ridge (alpha=    0.1): R² Train = 0.505694, R² Test = 0.020936\n",
            "Ridge (alpha=      1): R² Train = 0.505604, R² Test = 0.034035\n",
            "Ridge (alpha=     10): R² Train = 0.501086, R² Test = -0.115807\n",
            "Ridge (alpha=    100): R² Train = 0.438472, R² Test = -0.909931\n",
            "Ridge (alpha=   1000): R² Train = 0.344316, R² Test = 0.224886\n",
            "\n",
            "✓ Meilleur Ridge: alpha = 1000\n",
            "Ridge Polynomial Degree=4 (Train): R² = 0.344316, MAE = 8.4916, RMSE = 86.4420\n",
            "Ridge Polynomial Degree=4 (Test):  R² = 0.224886, MAE = 14.4636, RMSE = 142.5257\n",
            "\n",
            "================================================================================\n",
            "RÉSUMÉ COMPLET: TOUS LES MODÈLES\n",
            "================================================================================\n",
            "                                           Model  R² Train    R² Test MAE Train   MAE Test\n",
            "                               Linear Regression  0.002881   0.016390    9.1718  15.318206\n",
            "                Polynomial (Degree=4) - Original  0.505722 -21.394002  3.923515  43.116911\n",
            "              Polynomial (Degree=2) - Solution A  0.375404   0.757450  5.197002    7.30389\n",
            "Ridge Polynomial (Degree=4, α=1000) - Solution B  0.344316   0.224886   8.49161  14.463606\n",
            "                        RandomForest (Augmented)  0.752566   0.939710                     \n",
            "                    GradientBoosting (Augmented)  0.793688   0.936364                     \n",
            "\n",
            "================================================================================\n",
            "✓ MEILLEUR MODÈLE GLOBAL: RandomForest\n",
            "  R² (Test) = 0.939710\n",
            "================================================================================\n",
            "✓✓ Le modèle DÉPASSSE le seuil R² > 0.80 ✓✓\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "IMPACT DES SOLUTIONS SUR LA RÉGRESSION POLYNOMIALE\n",
            "--------------------------------------------------------------------------------\n",
            "Polynomial Degree=4 (Original):  R² Test = -21.394002 ❌ (Surapprentissage)\n",
            "Polynomial Degree=2 (Sol. A):    R² Test = 0.757450 ✓\n",
            "Ridge Polynomial Deg=4 (Sol. B): R² Test = 0.224886 ✓\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ========================================\n",
        "# AMÉLIORATION: Solutions pour Régression Polynomiale\n",
        "# Solution A: Réduire le degré polynomial de 4 à 2\n",
        "# Solution B: Utiliser Ridge regularization sur le polynôme (degree=4)\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AMÉLIORATION: SOLUTIONS POUR LA RÉGRESSION POLYNOMIALE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- SOLUTION A: Réduire le degré à 2 ---\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SOLUTION A: Polynomial Regression (Degree=2)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "poly_deg2 = PolynomialFeatures(degree=2)\n",
        "X_train_poly_deg2 = poly_deg2.fit_transform(X_train_best)\n",
        "X_test_poly_deg2 = poly_deg2.transform(X_test_best)\n",
        "\n",
        "poly_model_deg2 = LinearRegression()\n",
        "poly_model_deg2.fit(X_train_poly_deg2, y_train_best)\n",
        "\n",
        "y_train_poly_deg2 = poly_model_deg2.predict(X_train_poly_deg2)\n",
        "y_test_poly_deg2 = poly_model_deg2.predict(X_test_poly_deg2)\n",
        "\n",
        "r2_train_poly_deg2 = r2_score(y_train_best, y_train_poly_deg2)\n",
        "r2_test_poly_deg2 = r2_score(y_test_best, y_test_poly_deg2)\n",
        "mae_train_poly_deg2 = mean_absolute_error(y_train_best, y_train_poly_deg2)\n",
        "mae_test_poly_deg2 = mean_absolute_error(y_test_best, y_test_poly_deg2)\n",
        "rmse_train_poly_deg2 = np.sqrt(mean_squared_error(y_train_best, y_train_poly_deg2))\n",
        "rmse_test_poly_deg2 = np.sqrt(mean_squared_error(y_test_best, y_test_poly_deg2))\n",
        "\n",
        "print(f\"Polynomial (Degree=2) Features shape: {X_train_poly_deg2.shape}\")\n",
        "print(f\"Polynomial Regression Degree=2 (Train): R² = {r2_train_poly_deg2:.6f}, MAE = {mae_train_poly_deg2:.4f}, RMSE = {rmse_train_poly_deg2:.4f}\")\n",
        "print(f\"Polynomial Regression Degree=2 (Test):  R² = {r2_test_poly_deg2:.6f}, MAE = {mae_test_poly_deg2:.4f}, RMSE = {rmse_test_poly_deg2:.4f}\")\n",
        "\n",
        "# --- SOLUTION B: Ridge Regression sur Polynomial (degree=4) ---\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SOLUTION B: Ridge Regularization on Polynomial (Degree=4)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Créer les features polynomiales (degree=4)\n",
        "poly_deg4_ridge = PolynomialFeatures(degree=4)\n",
        "X_train_poly_deg4_ridge = poly_deg4_ridge.fit_transform(X_train_best)\n",
        "X_test_poly_deg4_ridge = poly_deg4_ridge.transform(X_test_best)\n",
        "\n",
        "# Tester plusieurs valeurs d'alpha pour Ridge\n",
        "ridge_alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "ridge_results = []\n",
        "\n",
        "for alpha in ridge_alphas:\n",
        "    ridge_poly = Ridge(alpha=alpha)\n",
        "    ridge_poly.fit(X_train_poly_deg4_ridge, y_train_best)\n",
        "\n",
        "    y_train_ridge_poly = ridge_poly.predict(X_train_poly_deg4_ridge)\n",
        "    y_test_ridge_poly = ridge_poly.predict(X_test_poly_deg4_ridge)\n",
        "\n",
        "    r2_train = r2_score(y_train_best, y_train_ridge_poly)\n",
        "    r2_test = r2_score(y_test_best, y_test_ridge_poly)\n",
        "\n",
        "    ridge_results.append({\n",
        "        'alpha': alpha,\n",
        "        'R2_train': r2_train,\n",
        "        'R2_test': r2_test,\n",
        "        'model': ridge_poly,\n",
        "        'y_train_pred': y_train_ridge_poly,\n",
        "        'y_test_pred': y_test_ridge_poly\n",
        "    })\n",
        "\n",
        "    print(f\"Ridge (alpha={alpha:>7}): R² Train = {r2_train:>8.6f}, R² Test = {r2_test:>8.6f}\")\n",
        "\n",
        "# Sélectionner le meilleur Ridge (basé sur R² test)\n",
        "best_ridge_idx = max(range(len(ridge_results)), key=lambda i: ridge_results[i]['R2_test'])\n",
        "best_ridge_result = ridge_results[best_ridge_idx]\n",
        "best_ridge_alpha = best_ridge_result['alpha']\n",
        "best_ridge_model = best_ridge_result['model']\n",
        "\n",
        "r2_train_ridge_poly = best_ridge_result['R2_train']\n",
        "r2_test_ridge_poly = best_ridge_result['R2_test']\n",
        "y_train_ridge_poly_best = best_ridge_result['y_train_pred']\n",
        "y_test_ridge_poly_best = best_ridge_result['y_test_pred']\n",
        "\n",
        "mae_train_ridge_poly = mean_absolute_error(y_train_best, y_train_ridge_poly_best)\n",
        "mae_test_ridge_poly = mean_absolute_error(y_test_best, y_test_ridge_poly_best)\n",
        "rmse_train_ridge_poly = np.sqrt(mean_squared_error(y_train_best, y_train_ridge_poly_best))\n",
        "rmse_test_ridge_poly = np.sqrt(mean_squared_error(y_test_best, y_test_ridge_poly_best))\n",
        "\n",
        "print(f\"\\n✓ Meilleur Ridge: alpha = {best_ridge_alpha}\")\n",
        "print(f\"Ridge Polynomial Degree=4 (Train): R² = {r2_train_ridge_poly:.6f}, MAE = {mae_train_ridge_poly:.4f}, RMSE = {rmse_train_ridge_poly:.4f}\")\n",
        "print(f\"Ridge Polynomial Degree=4 (Test):  R² = {r2_test_ridge_poly:.6f}, MAE = {mae_test_ridge_poly:.4f}, RMSE = {rmse_test_ridge_poly:.4f}\")\n",
        "\n",
        "# --- RÉSUMÉ COMPLET: COMPARAISON DE TOUS LES MODÈLES ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RÉSUMÉ COMPLET: TOUS LES MODÈLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_models_summary = [\n",
        "    ['Linear Regression', r2_train_lin, r2_test_lin, mae_train_lin, mae_test_lin],\n",
        "    ['Polynomial (Degree=4) - Original', r2_train_poly, r2_test_poly, mae_train_poly, mae_test_poly],\n",
        "    ['Polynomial (Degree=2) - Solution A', r2_train_poly_deg2, r2_test_poly_deg2, mae_train_poly_deg2, mae_test_poly_deg2],\n",
        "    [f'Ridge Polynomial (Degree=4, α={best_ridge_alpha}) - Solution B', r2_train_ridge_poly, r2_test_ridge_poly, mae_train_ridge_poly, mae_test_ridge_poly],\n",
        "    ['RandomForest (Augmented)', r2_train_rf_best, r2_test_rf_best, '', ''],\n",
        "    ['GradientBoosting (Augmented)', r2_train_gb_best, r2_test_gb_best, '', '']\n",
        "]\n",
        "\n",
        "all_models_df = pd.DataFrame(all_models_summary, columns=['Model', 'R² Train', 'R² Test', 'MAE Train', 'MAE Test'])\n",
        "print(all_models_df.to_string(index=False))\n",
        "\n",
        "# Meilleur modèle global\n",
        "test_r2_all = [r2_test_lin, r2_test_poly, r2_test_poly_deg2, r2_test_ridge_poly, r2_test_rf_best, r2_test_gb_best]\n",
        "best_global_idx = test_r2_all.index(max(test_r2_all))\n",
        "best_models_all = ['Linear Regression', 'Polynomial (Degree=4)', 'Polynomial (Degree=2)',\n",
        "                   'Ridge Polynomial', 'RandomForest', 'GradientBoosting']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"✓ MEILLEUR MODÈLE GLOBAL: {best_models_all[best_global_idx]}\")\n",
        "print(f\"  R² (Test) = {max(test_r2_all):.6f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if max(test_r2_all) > 0.80:\n",
        "    print(\"✓✓ Le modèle DÉPASSSE le seuil R² > 0.80 ✓✓\")\n",
        "elif max(test_r2_all) > 0.70:\n",
        "    print(\"✓ Le modèle atteint R² > 0.70 (très bon)\")\n",
        "else:\n",
        "    print(f\"⚠ R² = {max(test_r2_all):.6f} (à améliorer)\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- Visualisation: Impact des solutions ---\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"IMPACT DES SOLUTIONS SUR LA RÉGRESSION POLYNOMIALE\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Polynomial Degree=4 (Original):  R² Test = {r2_test_poly:.6f} ❌ (Surapprentissage)\")\n",
        "print(f\"Polynomial Degree=2 (Sol. A):    R² Test = {r2_test_poly_deg2:.6f} {('✓' if r2_test_poly_deg2 > r2_test_poly else '❌')}\")\n",
        "print(f\"Ridge Polynomial Deg=4 (Sol. B): R² Test = {r2_test_ridge_poly:.6f} {('✓' if r2_test_ridge_poly > r2_test_poly else '❌')}\")\n",
        "print(\"-\"*80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oXk41AhKnFk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6b676a6"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Save the best model and features ---\n",
        "\n",
        "# Best RandomForest model\n",
        "with open('best_random_forest_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_rf_final, file)\n",
        "\n",
        "# Best features\n",
        "with open('best_features.pkl', 'wb') as file:\n",
        "    pickle.dump(best_features, file)\n",
        "\n",
        "print(\"Best RandomForest model and features saved to 'best_random_forest_model.pkl' and 'best_features.pkl'.\")\n",
        "\n",
        "# --- 2. Création du DataFrame de Sortie ---\n",
        "\n",
        "# Inverse transform X_test_best to get original feature values\n",
        "X_test_original_features = scaler_best.inverse_transform(X_test_best)\n",
        "\n",
        "# Create a DataFrame for original test features\n",
        "predictions_df = pd.DataFrame(X_test_original_features, columns=best_features)\n",
        "\n",
        "# Add true target values\n",
        "predictions_df['True_Nombre_Salaries'] = y_test_best.values\n",
        "\n",
        "# Add predicted target values from the best RandomForest model\n",
        "predictions_df['Predicted_Nombre_Salaries'] = y_test_rf_best\n",
        "\n",
        "print(\"\\nPredictions DataFrame created successfully.\")\n",
        "print(predictions_df.head())\n",
        "\n",
        "# --- 3. Exportation du DataFrame de prédictions dans un fichier CSV ---\n",
        "predictions_df.to_csv('model_predictions.csv', index=False)\n",
        "print(\"\\nPredictions DataFrame exported to 'model_predictions.csv'.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}